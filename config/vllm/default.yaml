use_mock: false  # Set to true to use mock vLLM for faster debugging

mm_encoder_tp_mode: "data"
mm_processor_cache_gb: 0  # Cache for processed multimodal inputs (0 = disabled)
seed: 0
dtype: "bfloat16"
gpu_memory_utilization: 0.9
enforce_eager: false
max_model_len: -1 # -1 auto-fits to GPU memory, null uses model default
max_num_batched_tokens: null
trust_remote_code: true
async_scheduling: true
skip_mm_profiling: false

enable_prefix_caching: true  # Enable automatic prefix caching for shared prompts

mm_processor_kwargs:
  do_resize: false
  do_sample_frames: false

# NOTE: limit_mm_per_prompt.video is computed dynamically in engine.py
# based on cfg.prompt.num_shots (num_shots + 1)
limit_mm_per_prompt:
  image: 0
  video: 1  # Default for zero-shot; overridden dynamically for few-shot

enable_expert_parallel: null  # by default auto-detect MoE models

tensor_parallel_size: null # use all GPUs by default
