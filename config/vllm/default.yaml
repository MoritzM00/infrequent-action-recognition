use_mock: false  # Set to true to use mock vLLM for faster debugging

mm_encoder_tp_mode: "data"
mm_processor_cache_gb: 0
seed: 0
dtype: "bfloat16"
gpu_memory_utilization: 0.9
enforce_eager: false
max_model_len: -1 # -1 auto-fits to GPU memory, null uses model default
max_num_batched_tokens: null
trust_remote_code: true
async_scheduling: true
skip_mm_profiling: false

mm_processor_kwargs:
  do_resize: false
  do_sample_frames: false

limit_mm_per_prompt:
  image: 0
  video: 1

enable_expert_parallel: null  # by default auto-detect MoE models

tensor_parallel_size: null # use all GPUs by default
