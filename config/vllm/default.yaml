use_mock: false  # Set to true to use mock vLLM for faster debugging

mm_encoder_tp_mode: "data"
mm_processor_cache_gb: 0
seed: 0
dtype: "bfloat16"
gpu_memory_utilization: 0.9
enforce_eager: false
max_model_len: 32000 # use model default, or override based on VRAM constraints
max_num_batched_tokens: null
trust_remote_code: true
async_scheduling: false
skip_mm_profiling: false

mm_processor_kwargs:
  do_resize: true
  do_sample_frames: false

limit_mm_per_prompt:
  image: 0
  video: 1

enable_expert_parallel: false  # Set to true for MoE models

tensor_parallel_size: null # use all GPUs by default
