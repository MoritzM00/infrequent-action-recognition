# vLLM configuration with sensible defaults
# tensor_parallel_size is set automatically based on model size in the script
mm_encoder_tp_mode: "data"
mm_processor_cache_gb: 0
seed: 0
dtype: "bfloat16"
gpu_memory_utilization: 0.9
mm_processor_kwargs:
  min_pixels: 16384   # 16 * 32 * 32
  max_pixels: 409600  # 400 * 32 * 32

# Optional overrides for special cases
enable_expert_parallel: false  # Set to true for MoE models

tensor_parallel_size: null # use all GPUs by default
