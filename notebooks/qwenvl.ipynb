{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55171d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OMNIFALL_ROOT = \"/lsdf/data/activity/fall_detection/cvhci_fall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infreqact.data.video_dataset import OmnifallVideoDataset\n",
    "\n",
    "dataset_config = {\n",
    "    \"video_root\": f\"{OMNIFALL_ROOT}/OOPS/video\",\n",
    "    \"annotations_file\": \"hf://simplexsigil2/omnifall/labels/OOPS.csv\",\n",
    "    \"split_root\": \"hf://simplexsigil2/omnifall/splits\",\n",
    "    \"dataset_name\": \"OOPS\",\n",
    "    \"mode\": \"test\",  # Start with test set (smaller)\n",
    "    \"split\": \"cs\",  # Cross-subject split\n",
    "    \"target_fps\": 8.0,  # Low FPS for quick testing\n",
    "    \"vid_frame_count\": 16,\n",
    "    \"data_fps\": 30.0,  # OOPS videos are 30 FPS\n",
    "    \"ext\": \".mp4\",\n",
    "    \"fast\": True,\n",
    "}\n",
    "\n",
    "print(\"\\nDataset Configuration:\")\n",
    "for key, value in dataset_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nCreating OmnifallVideoDataset...\")\n",
    "try:\n",
    "    dataset = OmnifallVideoDataset(**dataset_config)\n",
    "    print(\"✓ Dataset created successfully!\")\n",
    "    print(f\"\\n{dataset}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to create dataset: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "frames = [Image.fromarray(frame) for frame in sample[\"video\"]]\n",
    "frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "\n",
    "model_checkpoint = \"Qwen/Qwen3-VL-2B-Thinking\"\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    do_sample_frames=False,\n",
    "    size={\"shortest_edge\": 4 * 32 * 32, \"longest_edge\": 256 * 32 * 32 * 2},\n",
    ")\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20321f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# budget for image processor, since the compression ratio is 32 for Qwen3-VL, we can set the number of visual tokens of a single image to 256-1280 (32× spatial compression)\n",
    "# processor.image_processor.size = {\"longest_edge\": 1280*32*32, \"shortest_edge\": 256*32*32}\n",
    "\n",
    "# budget for video processor, we can set the number of visual tokens of a single video to 256-16384 (32× spatial compression + 2× temporal compression)\n",
    "# processor.video_processor.size = {\"longest_edge\": 16384*32*32*2, \"shortest_edge\": 256*32*32*2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f078fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.video_utils import VideoMetadata\n",
    "\n",
    "metadata = VideoMetadata(\n",
    "    total_num_frames=len(frames),\n",
    "    fps=8.0,\n",
    "    frames_indices=list(range(len(frames))),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": frames,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe the action happening in the video.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    video_metadata=metadata,\n",
    ").to(model.device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a15ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"pixel_values_videos\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2)\n",
    "for batch in loader:\n",
    "    print(batch[\"video\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3fb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
