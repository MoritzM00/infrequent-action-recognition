{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55171d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OMNIFALL_ROOT = \"/lsdf/data/activity/fall_detection/cvhci_fall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infreqact.data.video_dataset import OmnifallVideoDataset\n",
    "\n",
    "dataset_config = {\n",
    "    \"video_root\": f\"{OMNIFALL_ROOT}/OOPS/video\",\n",
    "    \"annotations_file\": \"hf://simplexsigil2/omnifall/labels/OOPS.csv\",\n",
    "    \"split_root\": \"hf://simplexsigil2/omnifall/splits\",\n",
    "    \"dataset_name\": \"OOPS\",\n",
    "    \"mode\": \"test\",  # Start with test set (smaller)\n",
    "    \"split\": \"cs\",  # Cross-subject split\n",
    "    \"target_fps\": 8.0,  # Low FPS for quick testing\n",
    "    \"vid_frame_count\": 16,\n",
    "    \"data_fps\": 30.0,  # OOPS videos are 30 FPS\n",
    "    \"ext\": \".mp4\",\n",
    "    \"fast\": True,\n",
    "}\n",
    "\n",
    "print(\"\\nDataset Configuration:\")\n",
    "for key, value in dataset_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nCreating OmnifallVideoDataset...\")\n",
    "try:\n",
    "    dataset = OmnifallVideoDataset(**dataset_config)\n",
    "    print(\"✓ Dataset created successfully!\")\n",
    "    print(f\"\\n{dataset}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to create dataset: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "frames = [Image.fromarray(frame) for frame in sample[\"video\"]]\n",
    "frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "\n",
    "model_checkpoint = \"OpenGVLab/InternVL3_5-1B-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_checkpoint, do_sample_frames=False)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_checkpoint, device_map=\"auto\", dtype=\"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.video_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d92408",
   "metadata": {},
   "outputs": [],
   "source": [
    "R1_SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI assistant that rigorously follows this response protocol:\n",
    "\n",
    "1. First, conduct a detailed analysis of the question. Consider different angles, potential solutions, and reason through the problem step-by-step. Enclose this entire thinking process within <think> and </think> tags.\n",
    "\n",
    "2. After the thinking section, provide a clear, concise, and direct answer to the user's question. Separate the answer from the think section with a newline.\n",
    "\n",
    "Ensure that the thinking process is thorough but remains focused on the query. The final answer should be standalone and not reference the thinking section.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": R1_SYSTEM_PROMPT}]},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": frames,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe the action happening in the video.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    video_metadata={\n",
    "        \"fps\": 8.0,\n",
    "        \"total_num_frames\": len(frames),\n",
    "        \"frames_indices\": list(range(len(frames))),\n",
    "    },\n",
    ").to(model.device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e2227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cdbb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
