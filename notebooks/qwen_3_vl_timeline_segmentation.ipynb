{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "07a5e83c"
   },
   "source": [
    "### Video Understanding with Qwen3-VL\n",
    "\n",
    "In this notebook, we delve into the capabilities of the **Qwen3-VL** model for video understanding tasks. Our objective is to showcase how this advanced model can be applied to various video analysis scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "223b154d"
   },
   "source": [
    "#### \\[Setup\\]\n",
    "\n",
    "We start by loading the pre-trained `Qwen3-VL` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OMNIFALL_ROOT = \"/lsdf/data/activity/fall_detection/cvhci_fall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALpi6elXIwLa",
    "outputId": "1f7a6e78-5722-46e9-bf65-4e4e8b436202"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "if major_version >= 8:\n",
    "    print(\"GPU is compatible with FlashAttention\")\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    print(\"GPU is not compatible with FlashAttention\")\n",
    "    attn_implementation = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infreqact.inference.base import inference, init_hf_qwen_model\n",
    "from PIL import Image\n",
    "\n",
    "model, processor = init_hf_qwen_model(\n",
    "    size=\"2B\", attn_implementation=attn_implementation, cache_dir=\"../.cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import smart_resize\n",
    "\n",
    "smart_resize(720, 1280, factor=32, min_pixels=16384, max_pixels=204800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infreqact.data.video_dataset import OmnifallVideoDataset\n",
    "\n",
    "dataset_config = {\n",
    "    \"video_root\": f\"{OMNIFALL_ROOT}/OOPS/video\",\n",
    "    \"annotations_file\": \"hf://simplexsigil2/omnifall/labels/OOPS.csv\",\n",
    "    \"split_root\": \"hf://simplexsigil2/omnifall/splits\",\n",
    "    \"dataset_name\": \"OOPS\",\n",
    "    \"mode\": \"test\",  # Start with test set (smaller)\n",
    "    \"split\": \"cs\",  # Cross-subject split\n",
    "    \"target_fps\": 8.0,  # Low FPS for quick testing\n",
    "    \"vid_frame_count\": 16,\n",
    "    \"data_fps\": 30.0,  # OOPS videos are 30 FPS\n",
    "    \"ext\": \".mp4\",\n",
    "    \"fast\": True,\n",
    "}\n",
    "\n",
    "print(\"\\nDataset Configuration:\")\n",
    "for key, value in dataset_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nCreating OmnifallVideoDataset...\")\n",
    "try:\n",
    "    dataset = OmnifallVideoDataset(**dataset_config)\n",
    "    print(\"✓ Dataset created successfully!\")\n",
    "    print(f\"\\n{dataset}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to create dataset: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [Image.fromarray(frame) for frame in inputs[\"video\"]]\n",
    "frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "9JEg2y2z9Liq",
    "outputId": "4b81fe50-d02e-446f-a59a-b9f0cee71ecf"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result, inputs = inference(\n",
    "    frames,\n",
    "    \"Briefly describe this video, focusing on the (infrequent) actions a person does, e.g. falling down or examining something\",\n",
    "    model,\n",
    "    processor,\n",
    "    max_pixels=500 * 32 * 32,\n",
    "    return_inputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "ec9ec460"
   },
   "source": [
    "### 1. Using Video URL - Local Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "VRPFBXVpM82c"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationInfo, field_validator, model_validator\n",
    "\n",
    "\n",
    "class ActionSegment(BaseModel):\n",
    "    label: Annotated[int, Field(ge=0, le=15)]\n",
    "    description: str\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "\n",
    "    @field_validator(\"end_time\", mode=\"after\")\n",
    "    @classmethod\n",
    "    def end_time_gt_start_time(cls, value: float, info: ValidationInfo):\n",
    "        if value <= info.data[\"start_time\"]:\n",
    "            raise ValueError(\"end_time must be greater than start_time\")\n",
    "        return value\n",
    "\n",
    "\n",
    "class ActionDetection(BaseModel):\n",
    "    segments: list[ActionSegment]\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_sorted_and_non_overlapping(self):\n",
    "        segs = sorted(self.segments, key=lambda s: s.start_time)\n",
    "        for i in range(len(segs) - 1):\n",
    "            if segs[i].end_time > segs[i + 1].start_time:\n",
    "                raise ValueError(\n",
    "                    f\"Segments overlap between index {i} and {i + 1}: \"\n",
    "                    f\"{segs[i].end_time} > {segs[i + 1].start_time}\"\n",
    "                )\n",
    "        self.segments = segs  # assign sorted version\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "xflxlZntRgiJ"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert in human activity recognition. Given a short video, segment it into temporally into continuous intervals, labeling each interval with one of the activity classes.\n",
    "\n",
    "TASK:\n",
    "Perform full temporal segmentation of the video into labeled segments. Each segment should have:\n",
    "- \"label\": the activity label ID (0 to 9, see below),\n",
    "- \"start_time\": when the activity starts (in seconds),\n",
    "- \"end_time\": when it ends (in seconds),\n",
    "- \"description\": short description of what happens in this segment\n",
    "\n",
    "ACTIVITY LABELS:\n",
    "0|walk - Move around, including jogging and running and \"drunk walking\", but only if it is not part of some special exercise like pulling your knees up. Not when pushing a large object like a chair, but included carrying something small like an apple.\n",
    "1|fall - The act of falling (from any previous state). Includes falling on a bed, if the process is not a controlled lying down with arms as support.\n",
    "2|fallen - Being on the ground or a mattress after a fall.\n",
    "3|sit_down - Sitting down on bed or chair or ground.\n",
    "4|sitting - Sitting on bed or chair or ground.\n",
    "5|lie_down - Lying down intentionally (in contrast to a fall) on ground or bed.\n",
    "6|lying - Being in a lying position (in bed or on the ground) after intentionally getting into that position.\n",
    "7|stand_up - Standing up from a fallen state, from lying or sitting. Includes getting from lying position into sitting position.\n",
    "8|standing - Standing around without walking.\n",
    "9|other - Any other activity, including e.g. walking while pushing an object like a chair.p\n",
    "\n",
    "DEFINITIONS:\n",
    "- Transient actions (e.g., sit_down, fall, lie_down, stand_up) mark short events.\n",
    "- Static states (e.g., sitting, lying, standing, fallen) indicate sustained postures.\n",
    "- “fall” means an unintentional loss of balance leading to a person ending up on the ground.\n",
    "- “fallen” refers to the state after a fall when the person remains on the ground.\n",
    "- “other” should be used for actions not fitting any defined class.\n",
    "\n",
    "Focus on the activity labels given. If a scene changes slightly, but the label is the same, merge segments so that no segment has the same label as the previous one.\n",
    "\n",
    "OUTPUT JSON FORMAT: (for example)\n",
    "{\n",
    "    segments: [\n",
    "    {\n",
    "        \"label\": 0-9,\n",
    "        \"start_time\": float,\n",
    "        \"end_time\": float,\n",
    "        \"description\": \"some text\"\n",
    "    },\n",
    "    ...\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
