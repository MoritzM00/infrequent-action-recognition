{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Detailed Analysis of the predictions for a single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infreqact.utils.predictions import load_predictions_jsonl\n",
    "\n",
    "# local path or from wandb (below)\n",
    "JSONL_FILENAME = \"InternVL3_5-8B-HF_OOPS_cs_20260115-202011.jsonl\"\n",
    "\n",
    "path = \"../outputs/zeroshot-v2/predictions/\" + JSONL_FILENAME\n",
    "metadata, predictions = load_predictions_jsonl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from infreqact.utils.wandb import load_run_from_wandb\n",
    "# PROJECT = \"fall-detection-zeroshot-v2\"\n",
    "# ENTITY = \"moritzm00\"\n",
    "\n",
    "# RUN_ID = \"osmbqtk7\" # reasoning run but with parsing errors\n",
    "# config, predictions = load_run_from_wandb(RUN_ID, PROJECT, ENTITY)\n",
    "# metadata = {\"config\": config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from infreqact.data.video_dataset_factory import get_video_datasets\n",
    "from infreqact.utils.predictions import extract_labels_for_metrics\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "y_true, y_pred = extract_labels_for_metrics(predictions)\n",
    "\n",
    "cfg = metadata[\"config\"]\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "random.seed(cfg.get(\"data\", {}).get(\"seed\") or 0)\n",
    "\n",
    "# optionally change dataset parameters for visualizations\n",
    "cfg.dataset.vid_frame_count = 16\n",
    "cfg.dataset.model_fps = 8\n",
    "\n",
    "dataset = get_video_datasets(cfg, mode=\"test\", split=\"cs\", size=400)\n",
    "\n",
    "preds = pd.DataFrame(predictions).rename(\n",
    "    columns={\"predicted_label\": \"y_pred\", \"label_str\": \"y_true\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_per_segment_sampled = cfg.dataset.vid_frame_count / cfg.dataset.model_fps\n",
    "s_per_segment_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "def video_to_image_grid(\n",
    "    video: torch.Tensor, nrow: int | None = None, padding: int = 2, normalize: bool = True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a video tensor to an image grid.\n",
    "\n",
    "    Args:\n",
    "        video: Tensor of shape (T, C, H, W) where T is number of frames\n",
    "        nrow: Number of images per row. If None, uses ceil(sqrt(T))\n",
    "        padding: Padding between images\n",
    "        normalize: Whether to normalize the output to [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        Image grid tensor of shape (C, H', W') suitable for display\n",
    "    \"\"\"\n",
    "    T, C, H, W = video.shape\n",
    "\n",
    "    if nrow is None:\n",
    "        nrow = int(torch.ceil(torch.sqrt(torch.tensor(T, dtype=torch.float))).item())\n",
    "\n",
    "    # Use torchvision's make_grid utility\n",
    "    grid = vutils.make_grid(video, nrow=nrow, padding=padding, normalize=normalize)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(predictions).rename(\n",
    "    columns={\"predicted_label\": \"y_pred\", \"label_str\": \"y_true\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(idx: int, figsize=(12, 8)):\n",
    "    video = dataset[idx][\"video\"]\n",
    "    predicted_label = y_pred[idx]\n",
    "    true_label = y_true[idx]\n",
    "    segment_duration = preds.loc[idx, \"segment_duration\"]\n",
    "\n",
    "    caption = f\"Predicted: {predicted_label}, True: {true_label} (Model sees {s_per_segment_sampled:.1f}s out of {segment_duration:.1f}s)\"\n",
    "\n",
    "    # print reasoning if available\n",
    "    reasoning = preds.loc[idx, \"reasoning\"]\n",
    "    if reasoning != \"\":\n",
    "        print(\"Reasoning:\\n\", reasoning)\n",
    "\n",
    "    grid = video_to_image_grid(video)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(caption)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined DataFrame for comparison\n",
    "df_compare = pd.DataFrame(\n",
    "    {\"label\": y_true + y_pred, \"type\": [\"True\"] * len(y_true) + [\"Predicted\"] * len(y_pred)}\n",
    ")\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Count plot comparing distributions\n",
    "ax = sns.countplot(data=df_compare, x=\"label\", hue=\"type\")  # , palette=[\"#2ecc71\", \"#e74c3c\"])\n",
    "plt.title(\"Distribution of Predicted vs True Labels\", fontsize=14)\n",
    "plt.xlabel(\"Action Label\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"center\")\n",
    "plt.legend(title=\"Label Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get unique labels in order of appearance\n",
    "labels = sorted(set(y_true) | set(y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# Normalize by row (true labels) to show recall per class\n",
    "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    square=True,\n",
    "    cbar_kws={\"label\": \"Proportion\"},\n",
    ")\n",
    "plt.title(\"Confusion Matrix (Row-Normalized)\", fontsize=14)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"center\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Segment Duration vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.segment_duration.hist(bins=30, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'correct' column to track predictions\n",
    "preds[\"correct\"] = preds[\"y_true\"] == preds[\"y_pred\"]\n",
    "\n",
    "preds[\"duration_bin\"] = pd.cut(\n",
    "    preds[\"segment_duration\"],\n",
    "    bins=[0, 2, 3, 4, 5, 10, float(\"inf\")],\n",
    "    labels=[\"0-2s\", \"2-3s\", \"3-4s\", \"4-5s\", \"5-10s\", \"10s+\"],\n",
    ")\n",
    "# Calculate accuracy per bin\n",
    "accuracy_by_duration = (\n",
    "    preds.groupby(\"duration_bin\", observed=True)[\"correct\"].agg([\"mean\", \"count\"]).reset_index()\n",
    ")\n",
    "accuracy_by_duration.columns = [\"Duration\", \"Accuracy\", \"Sample Count\"]\n",
    "\n",
    "# Plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for accuracy\n",
    "bars = sns.barplot(data=accuracy_by_duration, x=\"Duration\", y=\"Accuracy\", ax=ax1, color=\"steelblue\")\n",
    "ax1.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax1.set_xlabel(\"Segment Duration\", fontsize=12)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Add sample counts as text on bars\n",
    "for i, (acc, count) in enumerate(\n",
    "    zip(accuracy_by_duration[\"Accuracy\"], accuracy_by_duration[\"Sample Count\"])\n",
    "):\n",
    "    ax1.text(i, acc + 0.02, f\"n={count}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.title(\"Classification Accuracy by Segment Duration\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "class_metrics = []\n",
    "for label in labels:\n",
    "    true_mask = preds[\"y_true\"] == label\n",
    "    pred_mask = preds[\"y_pred\"] == label\n",
    "\n",
    "    tp = (true_mask & pred_mask).sum()\n",
    "    fp = (~true_mask & pred_mask).sum()\n",
    "    fn = (true_mask & ~pred_mask).sum()\n",
    "\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    class_metrics.append(\n",
    "        {\n",
    "            \"class\": label,\n",
    "            \"support\": true_mask.sum(),\n",
    "            \"recall\": recall,\n",
    "            \"precision\": precision,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "class_df = pd.DataFrame(class_metrics).sort_values(\"f1\")\n",
    "\n",
    "# Plot per-class F1 scores\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(class_df[\"class\"], class_df[\"f1\"])\n",
    "ax.set_xlabel(\"F1 Score\", fontsize=12)\n",
    "ax.set_title(\"Per-Class F1 Score\", fontsize=14)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# Add support counts\n",
    "for i, (f1, support) in enumerate(zip(class_df[\"f1\"], class_df[\"support\"])):\n",
    "    ax.text(f1 + 0.02, i, f\"n={support}\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show detailed metrics table\n",
    "print(\"\\nðŸ“Š Per-Class Metrics (sorted by F1):\")\n",
    "print(class_df.to_string(index=False, float_format=lambda x: f\"{x:.2f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion analysis: What does each class get confused with?\n",
    "print(\"ðŸ” Misclassification Analysis per Class:\\n\")\n",
    "\n",
    "for label in labels:\n",
    "    true_mask = preds[\"y_true\"] == label\n",
    "    errors = preds[true_mask & ~preds[\"correct\"]]\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        error_dist = errors[\"y_pred\"].value_counts().head(3)\n",
    "        total_true = true_mask.sum()\n",
    "        print(\n",
    "            f\"ã€{label}ã€‘ (n={total_true}, errors={len(errors)}, recall={1 - len(errors) / total_true:.1%})\"\n",
    "        )\n",
    "        for wrong_pred, count in error_dist.items():\n",
    "            pct = count / total_true * 100\n",
    "            print(f\"   â†’ predicted as '{wrong_pred}': {count} ({pct:.1f}%)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap showing top confusions (filtered to significant errors)\n",
    "# Create a DataFrame of error rates: what % of class X is predicted as class Y\n",
    "error_matrix = pd.DataFrame(0.0, index=labels, columns=labels)\n",
    "\n",
    "for true_label in labels:\n",
    "    true_mask = preds[\"y_true\"] == true_label\n",
    "    total = true_mask.sum()\n",
    "    if total > 0:\n",
    "        pred_counts = preds[true_mask][\"y_pred\"].value_counts()\n",
    "        for pred_label, count in pred_counts.items():\n",
    "            if pred_label != true_label:  # Only off-diagonal (errors)\n",
    "                error_matrix.loc[true_label, pred_label] = count / total * 100\n",
    "\n",
    "# Plot only significant confusions\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create custom annotations with % symbol\n",
    "annot_matrix = error_matrix.map(lambda x: f\"{x:.0f}%\" if x >= 5 else \"\")\n",
    "\n",
    "sns.heatmap(\n",
    "    error_matrix,\n",
    "    annot=annot_matrix,\n",
    "    fmt=\"s\",  # 's' for string format\n",
    "    cmap=\"Reds\",\n",
    "    cbar_kws={\"label\": \"Error Rate (%)\"},\n",
    "    mask=(error_matrix < 5),  # Hide small errors for clarity\n",
    "    linewidths=0.5,\n",
    ")\n",
    "plt.title(\"Confusion Error Rates (showing only â‰¥5%)\", fontsize=14)\n",
    "plt.xlabel(\"Predicted As\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_by_class(\n",
    "    actual_class: str = None, predicted_class: str = None, sample_idx: int = 0, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize predictions filtered by actual and/or predicted class.\n",
    "\n",
    "    Args:\n",
    "        actual_class: Filter by ground truth label\n",
    "        predicted_class: Filter by predicted label\n",
    "        sample_idx: Which sample to show (0 = first match, 1 = second match, etc.)\n",
    "\n",
    "    Examples:\n",
    "        visualize_by_class(actual_class=\"fall\")  # Show a sample where true label is \"fall\"\n",
    "        visualize_by_class(predicted_class=\"fall\")  # Show a sample predicted as \"fall\"\n",
    "        visualize_by_class(actual_class=\"jump\", predicted_class=\"fall\")  # Show \"jump\" misclassified as \"fall\"\n",
    "    \"\"\"\n",
    "    if actual_class is None and predicted_class is None:\n",
    "        raise ValueError(\"Must specify at least one of actual_class or predicted_class\")\n",
    "\n",
    "    # Build filter mask\n",
    "    mask = pd.Series([True] * len(preds))\n",
    "\n",
    "    if actual_class is not None:\n",
    "        mask &= preds[\"y_true\"] == actual_class\n",
    "    if predicted_class is not None:\n",
    "        mask &= preds[\"y_pred\"] == predicted_class\n",
    "\n",
    "    matching_indices = preds[mask].index.tolist()\n",
    "\n",
    "    if len(matching_indices) == 0:\n",
    "        filter_desc = []\n",
    "        if actual_class:\n",
    "            filter_desc.append(f\"actual='{actual_class}'\")\n",
    "        if predicted_class:\n",
    "            filter_desc.append(f\"predicted='{predicted_class}'\")\n",
    "        print(f\"No samples found with {' and '.join(filter_desc)}\")\n",
    "        return\n",
    "\n",
    "    if sample_idx >= len(matching_indices):\n",
    "        print(\n",
    "            f\"Only {len(matching_indices)} samples match. Use sample_idx < {len(matching_indices)}\"\n",
    "        )\n",
    "        sample_idx = len(matching_indices) - 1\n",
    "\n",
    "    idx = matching_indices[sample_idx]\n",
    "\n",
    "    # Show info\n",
    "    filter_desc = []\n",
    "    if actual_class:\n",
    "        filter_desc.append(f\"actual='{actual_class}'\")\n",
    "    if predicted_class:\n",
    "        filter_desc.append(f\"predicted='{predicted_class}'\")\n",
    "    print(\n",
    "        f\"Showing sample {sample_idx + 1}/{len(matching_indices)} matching {' and '.join(filter_desc)}\"\n",
    "    )\n",
    "\n",
    "    visualize_prediction(idx, **kwargs)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# visualize_by_class(actual_class=\"jump\", predicted_class=\"fall\")  # Show jumpâ†’fall confusion\n",
    "# visualize_by_class(actual_class=\"fall\", predicted_class=\"fall\")  # Show correct fall prediction\n",
    "# visualize_by_class(predicted_class=\"crawl\")  # Show anything predicted as crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_by_class(predicted_class=\"kneeling\", figsize=(8, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_by_class(\n",
    "    actual_class=\"jump\", predicted_class=\"fall\", sample_idx=0\n",
    ")  # Show jumpâ†’fall confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_by_class(actual_class=\"fallen\", predicted_class=\"fall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
