{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Detailed Analysis of the predictions for a single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from infreqact.utils.predictions import load_predictions_jsonl\n",
    "\n",
    "# # local path or from wandb (below)\n",
    "# JSONL_FILENAME = \"InternVL3_5-8B-HF_OOPS_cs_20260115-202011.jsonl\"\n",
    "\n",
    "# path = \"../outputs/zeroshot-v2/predictions/\" + JSONL_FILENAME\n",
    "# metadata, predictions = load_predictions_jsonl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infreqact.utils.wandb import load_run_from_wandb\n",
    "\n",
    "PROJECT = \"fall-detection-zeroshot-v3\"\n",
    "ENTITY = \"moritzm00\"\n",
    "\n",
    "RUN_ID = \"wycr9tin\"  # 38B Internvl zeroshot\n",
    "# RUN_ID = \"s7b288b5\" # 8B COT\n",
    "config, predictions = load_run_from_wandb(RUN_ID, PROJECT, ENTITY)\n",
    "metadata = {\"config\": config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from infreqact.data.video_dataset_factory import get_video_datasets\n",
    "from infreqact.utils.predictions import extract_labels_for_metrics\n",
    "\n",
    "sns.set_theme(\n",
    "    style=\"white\",\n",
    "    context=\"paper\",\n",
    "    rc={\"text.usetex\": True, \"font.family\": \"serif\", \"font.serif\": [\"Computer Modern Roman\"]},\n",
    ")\n",
    "\n",
    "y_true, y_pred = extract_labels_for_metrics(predictions)\n",
    "\n",
    "cfg = metadata[\"config\"]\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "# optionally change dataset parameters for visualizations\n",
    "cfg.dataset.vid_frame_count = 16\n",
    "cfg.dataset.model_fps = 8\n",
    "\n",
    "seed = cfg.data.seed\n",
    "dataset = get_video_datasets(cfg, mode=\"test\", split=\"cs\", size=768, seed=seed)\n",
    "\n",
    "preds = pd.DataFrame(predictions).rename(\n",
    "    columns={\"predicted_label\": \"y_pred\", \"label_str\": \"y_true\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "def video_to_image_grid(\n",
    "    video: torch.Tensor, nrow: int | None = None, padding: int = 2, normalize: bool = True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a video tensor to an image grid.\n",
    "\n",
    "    Args:\n",
    "        video: Tensor of shape (T, C, H, W) where T is number of frames\n",
    "        nrow: Number of images per row. If None, uses ceil(sqrt(T))\n",
    "        padding: Padding between images\n",
    "        normalize: Whether to normalize the output to [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        Image grid tensor of shape (C, H', W') suitable for display\n",
    "    \"\"\"\n",
    "    T, C, H, W = video.shape\n",
    "\n",
    "    if nrow is None:\n",
    "        nrow = int(torch.ceil(torch.sqrt(torch.tensor(T, dtype=torch.float))).item())\n",
    "\n",
    "    # Use torchvision's make_grid utility\n",
    "    grid = vutils.make_grid(video, nrow=nrow, padding=padding, normalize=normalize)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(\n",
    "    idx: int, figsize=(16, 8), title=False, nrow: int | None = 8, savepath: str | None = None\n",
    "):\n",
    "    logger.info(f\"Visualizing prediction for index: {idx}\")\n",
    "    video = dataset[idx][\"video\"]\n",
    "    predicted_label = y_pred[idx]\n",
    "    true_label = y_true[idx]\n",
    "    caption = f\"Predicted: {predicted_label}, True: {true_label}\"\n",
    "    logger.info(caption)\n",
    "\n",
    "    # print reasoning if available\n",
    "    reasoning = preds.loc[idx, \"reasoning\"]\n",
    "    if reasoning != \"\":\n",
    "        print(\"Reasoning:\\n\", reasoning)\n",
    "\n",
    "    grid = video_to_image_grid(video, nrow=nrow)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(caption)\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = predictions[122][\"video_path\"]\n",
    "full_path = dataset.datasets[0].format_path(path)\n",
    "full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "\n",
    "# Load the video at 8 FPS\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "vr = decord.VideoReader(full_path)\n",
    "original_fps = vr.get_avg_fps()\n",
    "total_frames = len(vr)\n",
    "duration = total_frames / original_fps\n",
    "\n",
    "# Calculate frame indices for 8 FPS\n",
    "target_fps = 8\n",
    "frame_interval = original_fps / target_fps\n",
    "frame_indices = np.arange(0, total_frames, frame_interval).astype(int)\n",
    "frame_indices = frame_indices[frame_indices < total_frames]\n",
    "\n",
    "# Get timestamps for each sampled frame\n",
    "timestamps = frame_indices / original_fps\n",
    "\n",
    "# Load frames\n",
    "frames = vr.get_batch(frame_indices)  # (T, H, W, C)\n",
    "\n",
    "print(f\"Original FPS: {original_fps:.2f}, Duration: {duration:.2f}s\")\n",
    "print(f\"Sampled {len(frame_indices)} frames at {target_fps} FPS\")\n",
    "print(f\"Frame shape: {frames.shape}\")\n",
    "print(f\"Timestamps (first 10): {timestamps[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "\n",
    "# Get segment info from predictions\n",
    "pred = predictions[122]\n",
    "start_time = pred[\"start_time\"]\n",
    "end_time = pred[\"end_time\"]\n",
    "\n",
    "# Calculate frame indices for the segment at 8 FPS\n",
    "start_frame = int(start_time * original_fps)\n",
    "end_frame = int(end_time * original_fps)\n",
    "\n",
    "# Sample at target FPS within the segment\n",
    "segment_frame_indices = np.arange(start_frame, end_frame, frame_interval).astype(int)\n",
    "segment_frame_indices = segment_frame_indices[segment_frame_indices < total_frames]\n",
    "\n",
    "# Get frames for this segment\n",
    "segment_frames = vr.get_batch(segment_frame_indices)  # (T, H, W, C)\n",
    "\n",
    "print(f\"Segment: {start_time:.2f}s - {end_time:.2f}s (duration: {end_time - start_time:.2f}s)\")\n",
    "print(f\"Frames: {len(segment_frame_indices)} at {target_fps} FPS\")\n",
    "print(f\"True label: {pred['label_str']}, Predicted: {pred['predicted_label']}\")\n",
    "\n",
    "# Visualize as video using matplotlib animation\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Convert to numpy for display\n",
    "frames_np = segment_frames.numpy()\n",
    "\n",
    "im = ax.imshow(frames_np[0])\n",
    "ax.set_title(f\"True: {pred['label_str']} | Pred: {pred['predicted_label']}\")\n",
    "\n",
    "\n",
    "def update(frame_idx):\n",
    "    im.set_array(frames_np[frame_idx])\n",
    "    return [im]\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, update, frames=len(frames_np), interval=1000 / target_fps, blit=True\n",
    ")\n",
    "plt.close(fig)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize n random idx\n",
    "rng = np.random.default_rng(0)\n",
    "idx = rng.integers(low=0, high=len(dataset), size=8)\n",
    "for i in idx:\n",
    "    pred = predictions[i]\n",
    "    true_label = pred[\"label_str\"]\n",
    "    visualize_prediction(idx=i, nrow=8, title=False)\n",
    "    plt.savefig(f\"../outputs/plots/pred_{i}_{true_label}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(\n",
    "    45, nrow=8, savepath=\"../outputs/plots/pred_45_fall_wide.pdf\", figsize=(36, 12)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined DataFrame for comparison\n",
    "df_compare = pd.DataFrame(\n",
    "    {\"label\": y_true + y_pred, \"type\": [\"True\"] * len(y_true) + [\"Predicted\"] * len(y_pred)}\n",
    ")\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Count plot comparing distributions\n",
    "ax = sns.countplot(data=df_compare, x=\"label\", hue=\"type\")  # , palette=[\"#2ecc71\", \"#e74c3c\"])\n",
    "# plt.title(\"Distribution of Predicted vs True Labels\", fontsize=14)\n",
    "plt.xlabel(\"Action Label\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"center\")\n",
    "plt.legend(title=\"Label Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get unique labels in order of appearance\n",
    "labels = sorted(set(y_true) | set(y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# Normalize by row (true labels) to show recall per class\n",
    "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Create annotation matrix with percentage and count (only for values >= 3%)\n",
    "annot_matrix = np.empty_like(cm, dtype=object)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        if cm_normalized[i, j] >= 0.03:\n",
    "            annot_matrix[i, j] = f\"{cm_normalized[i, j]:.2f}\\n({cm[i, j]})\"\n",
    "        else:\n",
    "            annot_matrix[i, j] = \"\"\n",
    "\n",
    "# Create mask for values below 3%\n",
    "mask = cm_normalized < 0.03\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "with sns.axes_style(\"white\"):\n",
    "    sns.heatmap(\n",
    "        cm_normalized,\n",
    "        annot=annot_matrix,\n",
    "        fmt=\"\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        square=True,\n",
    "        cbar_kws={\"label\": \"Proportion\"},\n",
    "        mask=mask,\n",
    "    )\n",
    "# plt.title(\"Confusion Matrix (Row-Normalized)\", fontsize=14)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"center\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/plots/confusion_matrix.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Segment Duration vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.segment_duration.hist(bins=30, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'correct' column to track predictions\n",
    "preds[\"correct\"] = preds[\"y_true\"] == preds[\"y_pred\"]\n",
    "\n",
    "preds[\"duration_bin\"] = pd.cut(\n",
    "    preds[\"segment_duration\"],\n",
    "    bins=[0, 2, 3, 4, 5, 10, float(\"inf\")],\n",
    "    labels=[\"0-2s\", \"2-3s\", \"3-4s\", \"4-5s\", \"5-10s\", \"10s+\"],\n",
    ")\n",
    "# Calculate accuracy per bin\n",
    "accuracy_by_duration = (\n",
    "    preds.groupby(\"duration_bin\", observed=True)[\"correct\"].agg([\"mean\", \"count\"]).reset_index()\n",
    ")\n",
    "accuracy_by_duration.columns = [\"Duration\", \"Accuracy\", \"Sample Count\"]\n",
    "\n",
    "# Plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for accuracy\n",
    "bars = sns.barplot(data=accuracy_by_duration, x=\"Duration\", y=\"Accuracy\", ax=ax1, color=\"steelblue\")\n",
    "ax1.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax1.set_xlabel(\"Segment Duration\", fontsize=12)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Add sample counts as text on bars\n",
    "for i, (acc, count) in enumerate(\n",
    "    zip(accuracy_by_duration[\"Accuracy\"], accuracy_by_duration[\"Sample Count\"])\n",
    "):\n",
    "    ax1.text(i, acc + 0.02, f\"n={count}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.title(\"Classification Accuracy by Segment Duration\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "class_metrics = []\n",
    "for label in labels:\n",
    "    true_mask = preds[\"y_true\"] == label\n",
    "    pred_mask = preds[\"y_pred\"] == label\n",
    "\n",
    "    tp = (true_mask & pred_mask).sum()\n",
    "    fp = (~true_mask & pred_mask).sum()\n",
    "    fn = (true_mask & ~pred_mask).sum()\n",
    "\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    class_metrics.append(\n",
    "        {\n",
    "            \"class\": label,\n",
    "            \"support\": true_mask.sum(),\n",
    "            \"recall\": recall,\n",
    "            \"precision\": precision,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "class_df = pd.DataFrame(class_metrics).sort_values(\"f1\")\n",
    "\n",
    "# Plot per-class F1 scores\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(class_df[\"class\"], class_df[\"f1\"])\n",
    "ax.set_xlabel(\"F1 Score\")\n",
    "# ax.set_title(\"Per-Class F1 Score\", fontsize=14)\n",
    "ax.set_xlim(0, 0.8)\n",
    "\n",
    "# Add vertical grid lines\n",
    "ax.xaxis.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Add support counts\n",
    "for i, (f1, support) in enumerate(zip(class_df[\"f1\"], class_df[\"support\"])):\n",
    "    ax.text(f1 + 0.02, i, f\"n={support}\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/plots/per_class_f1.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Show detailed metrics table\n",
    "print(\"\\nðŸ“Š Per-Class Metrics (sorted by F1):\")\n",
    "print(class_df.to_string(index=False, float_format=lambda x: f\"{x:.2f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion analysis: What does each class get confused with?\n",
    "print(\"ðŸ” Misclassification Analysis per Class:\\n\")\n",
    "\n",
    "for label in labels:\n",
    "    true_mask = preds[\"y_true\"] == label\n",
    "    errors = preds[true_mask & ~preds[\"correct\"]]\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        error_dist = errors[\"y_pred\"].value_counts().head(3)\n",
    "        total_true = true_mask.sum()\n",
    "        print(\n",
    "            f\"ã€{label}ã€‘ (n={total_true}, errors={len(errors)}, recall={1 - len(errors) / total_true:.1%})\"\n",
    "        )\n",
    "        for wrong_pred, count in error_dist.items():\n",
    "            pct = count / total_true * 100\n",
    "            print(f\"   â†’ predicted as '{wrong_pred}': {count} ({pct:.1f}%)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap showing top confusions (filtered to significant errors)\n",
    "# Create a DataFrame of error rates: what % of class X is predicted as class Y\n",
    "error_matrix = pd.DataFrame(0.0, index=labels, columns=labels)\n",
    "\n",
    "for true_label in labels:\n",
    "    true_mask = preds[\"y_true\"] == true_label\n",
    "    total = true_mask.sum()\n",
    "    if total > 0:\n",
    "        pred_counts = preds[true_mask][\"y_pred\"].value_counts()\n",
    "        for pred_label, count in pred_counts.items():\n",
    "            if pred_label != true_label:  # Only off-diagonal (errors)\n",
    "                error_matrix.loc[true_label, pred_label] = count / total * 100\n",
    "\n",
    "# Plot only significant confusions\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create custom annotations with % symbol\n",
    "annot_matrix = error_matrix.map(lambda x: f\"{x:.0f}%\" if x >= 5 else \"\")\n",
    "\n",
    "sns.heatmap(\n",
    "    error_matrix,\n",
    "    annot=annot_matrix,\n",
    "    fmt=\"s\",  # 's' for string format\n",
    "    cmap=\"Reds\",\n",
    "    cbar_kws={\"label\": \"Error Rate (%)\"},\n",
    "    mask=(error_matrix < 5),  # Hide small errors for clarity\n",
    "    linewidths=0.5,\n",
    ")\n",
    "plt.title(r\"Confusion Error Rates (showing only $\\geq$5\\%)\", fontsize=14)\n",
    "plt.xlabel(\"Predicted As\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_by_class(\n",
    "    actual_class: str = None, predicted_class: str = None, sample_idx: int = 0, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize predictions filtered by actual and/or predicted class.\n",
    "\n",
    "    Args:\n",
    "        actual_class: Filter by ground truth label\n",
    "        predicted_class: Filter by predicted label\n",
    "        sample_idx: Which sample to show (0 = first match, 1 = second match, etc.)\n",
    "\n",
    "    Examples:\n",
    "        visualize_by_class(actual_class=\"fall\")  # Show a sample where true label is \"fall\"\n",
    "        visualize_by_class(predicted_class=\"fall\")  # Show a sample predicted as \"fall\"\n",
    "        visualize_by_class(actual_class=\"jump\", predicted_class=\"fall\")  # Show \"jump\" misclassified as \"fall\"\n",
    "    \"\"\"\n",
    "    if actual_class is None and predicted_class is None:\n",
    "        raise ValueError(\"Must specify at least one of actual_class or predicted_class\")\n",
    "\n",
    "    # Build filter mask\n",
    "    mask = pd.Series([True] * len(preds))\n",
    "\n",
    "    if actual_class is not None:\n",
    "        mask &= preds[\"y_true\"] == actual_class\n",
    "    if predicted_class is not None:\n",
    "        mask &= preds[\"y_pred\"] == predicted_class\n",
    "\n",
    "    matching_indices = preds[mask].index.tolist()\n",
    "\n",
    "    if len(matching_indices) == 0:\n",
    "        filter_desc = []\n",
    "        if actual_class:\n",
    "            filter_desc.append(f\"actual='{actual_class}'\")\n",
    "        if predicted_class:\n",
    "            filter_desc.append(f\"predicted='{predicted_class}'\")\n",
    "        print(f\"No samples found with {' and '.join(filter_desc)}\")\n",
    "        return\n",
    "\n",
    "    if sample_idx >= len(matching_indices):\n",
    "        print(\n",
    "            f\"Only {len(matching_indices)} samples match. Use sample_idx < {len(matching_indices)}\"\n",
    "        )\n",
    "        sample_idx = len(matching_indices) - 1\n",
    "\n",
    "    idx = matching_indices[sample_idx]\n",
    "\n",
    "    # Show info\n",
    "    filter_desc = []\n",
    "    if actual_class:\n",
    "        filter_desc.append(f\"actual='{actual_class}'\")\n",
    "    if predicted_class:\n",
    "        filter_desc.append(f\"predicted='{predicted_class}'\")\n",
    "    print(\n",
    "        f\"Showing sample {sample_idx + 1}/{len(matching_indices)} matching {' and '.join(filter_desc)}\"\n",
    "    )\n",
    "\n",
    "    visualize_prediction(idx, **kwargs)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# visualize_by_class(actual_class=\"jump\", predicted_class=\"fall\")  # Show jumpâ†’fall confusion\n",
    "# visualize_by_class(actual_class=\"fall\", predicted_class=\"fall\")  # Show correct fall prediction\n",
    "# visualize_by_class(predicted_class=\"crawl\")  # Show anything predicted as crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_by_class(predicted_class=\"crawl\", title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_by_class(\n",
    "    actual_class=\"jump\", predicted_class=\"fall\", sample_idx=0, nrow=8, figsize=(16, 8)\n",
    ")\n",
    "plt.savefig(\"../outputs/plots/pred_jump_fall.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_by_class(actual_class=\"fallen\", predicted_class=\"fall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu129_vllm14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
