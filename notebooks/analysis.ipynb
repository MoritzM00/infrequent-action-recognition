{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Detailed Analysis of the predictions for a single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from infreqact.utils.predictions import load_predictions_jsonl\n",
    "\n",
    "# # local path or from wandb (below)\n",
    "# JSONL_FILENAME = \"InternVL3_5-8B-HF_OOPS_cs_20260115-202011.jsonl\"\n",
    "\n",
    "# path = \"../outputs/zeroshot-v2/predictions/\" + JSONL_FILENAME\n",
    "# metadata, predictions = load_predictions_jsonl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from falldet.utils.wandb import load_run_from_wandb\n",
    "\n",
    "PROJECT = \"fall-detection-zeroshot-v3\"\n",
    "ENTITY = \"moritzm00\"\n",
    "\n",
    "RUN_ID = \"ll7krunc\"\n",
    "config, predictions = load_run_from_wandb(RUN_ID, PROJECT, ENTITY)\n",
    "metadata = {\"config\": config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from falldet.data.video_dataset_factory import get_video_datasets\n",
    "from falldet.utils.predictions import extract_labels_for_metrics\n",
    "\n",
    "sns.set_theme(\n",
    "    style=\"white\",\n",
    "    context=\"paper\",\n",
    "    rc={\"text.usetex\": True, \"font.family\": \"serif\", \"font.serif\": [\"Computer Modern Roman\"]},\n",
    ")\n",
    "\n",
    "y_true, y_pred = extract_labels_for_metrics(predictions)\n",
    "\n",
    "cfg = metadata[\"config\"]\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "# optionally change dataset parameters for visualizations\n",
    "cfg.dataset.vid_frame_count = 9\n",
    "\n",
    "seed = cfg.data.seed\n",
    "# seed = None\n",
    "dataset = get_video_datasets(cfg, mode=\"test\", split=\"cs\", seed=seed, size=cfg.data.size)\n",
    "\n",
    "preds = pd.DataFrame(predictions).rename(\n",
    "    columns={\"predicted_label\": \"y_pred\", \"label_str\": \"y_true\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from falldet.visualization import visualize_video\n",
    "\n",
    "\n",
    "def visualize_prediction(\n",
    "    idx: int, figsize=(16, 8), title=False, nrow: int | None = 8, savepath: str | None = None\n",
    "):\n",
    "    logger.info(f\"Visualizing prediction for index: {idx}\")\n",
    "    video = dataset[idx][\"video\"]\n",
    "    predicted_label = y_pred[idx]\n",
    "    true_label = y_true[idx]\n",
    "    caption = f\"Predicted: {predicted_label}, True: {true_label}\"\n",
    "    logger.info(caption)\n",
    "\n",
    "    # print reasoning if available\n",
    "    reasoning = preds.loc[idx, \"reasoning\"]\n",
    "    if reasoning != \"\":\n",
    "        print(\"Reasoning:\\n\", reasoning)\n",
    "\n",
    "    fig, ax = visualize_video(video, figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(caption)\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_segments = preds.query(\"segment_duration < 0.5\")\n",
    "short_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize n random idx\n",
    "rng = np.random.default_rng(0)\n",
    "idx = rng.integers(low=0, high=len(dataset), size=8)\n",
    "for i in idx:\n",
    "    pred = predictions[i]\n",
    "    true_label = pred[\"label_str\"]\n",
    "    visualize_prediction(idx=i, nrow=8, title=False)\n",
    "    # plt.savefig(f\"../outputs/plots/pred_{i}_{true_label}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(\n",
    "    45, nrow=8, savepath=\"../outputs/plots/pred_45_fall_wide.pdf\", figsize=(36, 12)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined DataFrame for comparison\n",
    "df_compare = pd.DataFrame(\n",
    "    {\"label\": y_true + y_pred, \"type\": [\"True\"] * len(y_true) + [\"Predicted\"] * len(y_pred)}\n",
    ")\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Count plot comparing distributions\n",
    "ax = sns.countplot(data=df_compare, x=\"label\", hue=\"type\")  # , palette=[\"#2ecc71\", \"#e74c3c\"])\n",
    "# plt.title(\"Distribution of Predicted vs True Labels\", fontsize=14)\n",
    "plt.xlabel(\"Action Label\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"center\")\n",
    "plt.legend(title=\"Label Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get unique labels in order of appearance\n",
    "labels = sorted(set(y_true) | set(y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# Normalize by row (true labels) to show recall per class\n",
    "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Create annotation matrix with percentage and count (only for values >= 3%)\n",
    "annot_matrix = np.empty_like(cm, dtype=object)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        if cm_normalized[i, j] >= 0.03:\n",
    "            annot_matrix[i, j] = f\"{cm_normalized[i, j]:.2f}\\n({cm[i, j]})\"\n",
    "        else:\n",
    "            annot_matrix[i, j] = \"\"\n",
    "\n",
    "# Create mask for values below 3%\n",
    "mask = cm_normalized < 0.03\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "with sns.axes_style(\"white\"):\n",
    "    sns.heatmap(\n",
    "        cm_normalized,\n",
    "        annot=annot_matrix,\n",
    "        fmt=\"\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        square=True,\n",
    "        cbar_kws={\"label\": \"Proportion\"},\n",
    "        mask=mask,\n",
    "    )\n",
    "# plt.title(\"Confusion Matrix (Row-Normalized)\", fontsize=14)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"center\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/plots/confusion_matrix.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for only \"fall\" and \"fallen\" rows\n",
    "fall_fallen_labels = [\"fall\", \"fallen\"]\n",
    "\n",
    "# Extract relevant rows from the existing cm_normalized and cm\n",
    "fall_fallen_indices = [labels.index(label) for label in fall_fallen_labels]\n",
    "cm_subset = cm[fall_fallen_indices, :]\n",
    "cm_normalized_subset = cm_normalized[fall_fallen_indices, :]\n",
    "\n",
    "# Create annotation matrix with percentage and count\n",
    "annot_matrix_subset = np.empty_like(cm_subset, dtype=object)\n",
    "for i in range(cm_subset.shape[0]):\n",
    "    for j in range(cm_subset.shape[1]):\n",
    "        if cm_normalized_subset[i, j] >= 0.03:\n",
    "            annot_matrix_subset[i, j] = f\"{cm_normalized_subset[i, j]:.2f}\\n({cm_subset[i, j]})\"\n",
    "        else:\n",
    "            annot_matrix_subset[i, j] = \"\"\n",
    "\n",
    "# Create mask for values below 3%\n",
    "mask_subset = cm_normalized_subset < 0.03\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "with sns.axes_style(\"white\"):\n",
    "    sns.heatmap(\n",
    "        cm_normalized_subset,\n",
    "        annot=annot_matrix_subset,\n",
    "        fmt=\"\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=labels,\n",
    "        yticklabels=fall_fallen_labels,\n",
    "        square=False,\n",
    "        cbar_kws={\"label\": \"Proportion\"},\n",
    "        mask=mask_subset,\n",
    "    )\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"center\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/plots/confusion_matrix_fall_fallen.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Segment Duration vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.segment_duration.hist(bins=30, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'correct' column to track predictions\n",
    "preds[\"correct\"] = preds[\"y_true\"] == preds[\"y_pred\"]\n",
    "\n",
    "preds[\"duration_bin\"] = pd.cut(\n",
    "    preds[\"segment_duration\"],\n",
    "    bins=[0, 0.5, 1, 1.5, 2, 3, 4, 5, 10, float(\"inf\")],\n",
    "    labels=[\"0-0.5s\", \"0.5-1s\", \"1-1.5s\", \"1.5-2s\", \"2-3s\", \"3-4s\", \"4-5s\", \"5-10s\", \"10s+\"],\n",
    ")\n",
    "# Calculate accuracy per bin\n",
    "accuracy_by_duration = (\n",
    "    preds.groupby(\"duration_bin\", observed=True)[\"correct\"].agg([\"mean\", \"count\"]).reset_index()\n",
    ")\n",
    "accuracy_by_duration.columns = [\"Duration\", \"Accuracy\", \"Sample Count\"]\n",
    "\n",
    "# Plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for accuracy\n",
    "bars = sns.barplot(data=accuracy_by_duration, x=\"Duration\", y=\"Accuracy\", ax=ax1, color=\"steelblue\")\n",
    "ax1.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax1.set_xlabel(\"Segment Duration\", fontsize=12)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Add sample counts as text on bars\n",
    "for i, (acc, count) in enumerate(\n",
    "    zip(accuracy_by_duration[\"Accuracy\"], accuracy_by_duration[\"Sample Count\"])\n",
    "):\n",
    "    ax1.text(i, acc + 0.02, f\"n={count}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.title(\"Classification Accuracy by Segment Duration\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/plots/accuracy_by_duration_with_padding.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "class_metrics = []\n",
    "for label in labels:\n",
    "    true_mask = preds[\"y_true\"] == label\n",
    "    pred_mask = preds[\"y_pred\"] == label\n",
    "\n",
    "    tp = (true_mask & pred_mask).sum()\n",
    "    fp = (~true_mask & pred_mask).sum()\n",
    "    fn = (true_mask & ~pred_mask).sum()\n",
    "\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    class_metrics.append(\n",
    "        {\n",
    "            \"class\": label,\n",
    "            \"support\": true_mask.sum(),\n",
    "            \"recall\": recall,\n",
    "            \"precision\": precision,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "class_df = pd.DataFrame(class_metrics).sort_values(\"f1\")\n",
    "\n",
    "# Plot per-class F1 scores\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(class_df[\"class\"], class_df[\"f1\"])\n",
    "ax.set_xlabel(\"F1 Score\")\n",
    "# ax.set_title(\"Per-Class F1 Score\", fontsize=14)\n",
    "ax.set_xlim(0, 0.75)\n",
    "\n",
    "# Add vertical grid lines\n",
    "ax.xaxis.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Add support counts\n",
    "for i, (f1, support) in enumerate(zip(class_df[\"f1\"], class_df[\"support\"])):\n",
    "    ax.text(f1 + 0.02, i, f\"n={support}\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/plots/per_class_f1.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Show detailed metrics table\n",
    "print(\"\\nðŸ“Š Per-Class Metrics (sorted by F1):\")\n",
    "print(class_df.to_string(index=False, float_format=lambda x: f\"{x:.2f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion analysis: What does each class get confused with?\n",
    "print(\"ðŸ” Misclassification Analysis per Class:\\n\")\n",
    "\n",
    "for label in labels:\n",
    "    true_mask = preds[\"y_true\"] == label\n",
    "    errors = preds[true_mask & ~preds[\"correct\"]]\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        error_dist = errors[\"y_pred\"].value_counts().head(3)\n",
    "        total_true = true_mask.sum()\n",
    "        print(\n",
    "            f\"ã€{label}ã€‘ (n={total_true}, errors={len(errors)}, recall={1 - len(errors) / total_true:.1%})\"\n",
    "        )\n",
    "        for wrong_pred, count in error_dist.items():\n",
    "            pct = count / total_true * 100\n",
    "            print(f\"   â†’ predicted as '{wrong_pred}': {count} ({pct:.1f}%)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap showing top confusions (filtered to significant errors)\n",
    "# Create a DataFrame of error rates: what % of class X is predicted as class Y\n",
    "error_matrix = pd.DataFrame(0.0, index=labels, columns=labels)\n",
    "\n",
    "for true_label in labels:\n",
    "    true_mask = preds[\"y_true\"] == true_label\n",
    "    total = true_mask.sum()\n",
    "    if total > 0:\n",
    "        pred_counts = preds[true_mask][\"y_pred\"].value_counts()\n",
    "        for pred_label, count in pred_counts.items():\n",
    "            if pred_label != true_label:  # Only off-diagonal (errors)\n",
    "                error_matrix.loc[true_label, pred_label] = count / total * 100\n",
    "\n",
    "# Plot only significant confusions\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create custom annotations with % symbol\n",
    "annot_matrix = error_matrix.map(lambda x: f\"{x:.0f}%\" if x >= 5 else \"\")\n",
    "\n",
    "sns.heatmap(\n",
    "    error_matrix,\n",
    "    annot=annot_matrix,\n",
    "    fmt=\"s\",  # 's' for string format\n",
    "    cmap=\"Reds\",\n",
    "    cbar_kws={\"label\": \"Error Rate\"},\n",
    "    mask=(error_matrix < 5),  # Hide small errors for clarity\n",
    "    linewidths=0.5,\n",
    ")\n",
    "plt.title(r\"Confusion Error Rates (showing only $\\geq$5\\%)\", fontsize=14)\n",
    "plt.xlabel(\"Predicted As\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_by_class(\n",
    "    actual_class: str = None, predicted_class: str = None, sample_idx: int = 0, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize predictions filtered by actual and/or predicted class.\n",
    "\n",
    "    Args:\n",
    "        actual_class: Filter by ground truth label\n",
    "        predicted_class: Filter by predicted label\n",
    "        sample_idx: Which sample to show (0 = first match, 1 = second match, etc.)\n",
    "\n",
    "    Examples:\n",
    "        visualize_by_class(actual_class=\"fall\")  # Show a sample where true label is \"fall\"\n",
    "        visualize_by_class(predicted_class=\"fall\")  # Show a sample predicted as \"fall\"\n",
    "        visualize_by_class(actual_class=\"jump\", predicted_class=\"fall\")  # Show \"jump\" misclassified as \"fall\"\n",
    "    \"\"\"\n",
    "    if actual_class is None and predicted_class is None:\n",
    "        raise ValueError(\"Must specify at least one of actual_class or predicted_class\")\n",
    "\n",
    "    # Build filter mask\n",
    "    mask = pd.Series([True] * len(preds))\n",
    "\n",
    "    if actual_class is not None:\n",
    "        mask &= preds[\"y_true\"] == actual_class\n",
    "    if predicted_class is not None:\n",
    "        mask &= preds[\"y_pred\"] == predicted_class\n",
    "\n",
    "    matching_indices = preds[mask].index.tolist()\n",
    "\n",
    "    if len(matching_indices) == 0:\n",
    "        filter_desc = []\n",
    "        if actual_class:\n",
    "            filter_desc.append(f\"actual='{actual_class}'\")\n",
    "        if predicted_class:\n",
    "            filter_desc.append(f\"predicted='{predicted_class}'\")\n",
    "        print(f\"No samples found with {' and '.join(filter_desc)}\")\n",
    "        return\n",
    "\n",
    "    if sample_idx >= len(matching_indices):\n",
    "        print(\n",
    "            f\"Only {len(matching_indices)} samples match. Use sample_idx < {len(matching_indices)}\"\n",
    "        )\n",
    "        sample_idx = len(matching_indices) - 1\n",
    "\n",
    "    idx = matching_indices[sample_idx]\n",
    "\n",
    "    # Show info\n",
    "    filter_desc = []\n",
    "    if actual_class:\n",
    "        filter_desc.append(f\"actual='{actual_class}'\")\n",
    "    if predicted_class:\n",
    "        filter_desc.append(f\"predicted='{predicted_class}'\")\n",
    "    print(\n",
    "        f\"Showing sample {sample_idx + 1}/{len(matching_indices)} matching {' and '.join(filter_desc)}\"\n",
    "    )\n",
    "\n",
    "    visualize_prediction(idx, **kwargs)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# visualize_by_class(actual_class=\"jump\", predicted_class=\"fall\")  # Show jumpâ†’fall confusion\n",
    "# visualize_by_class(actual_class=\"fall\", predicted_class=\"fall\")  # Show correct fall prediction\n",
    "# visualize_by_class(predicted_class=\"crawl\")  # Show anything predicted as crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_by_class(predicted_class=\"crawl\", title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_by_class(actual_class=\"fall\", predicted_class=\"fall\", sample_idx=1, figsize=(16, 8))\n",
    "plt.savefig(\"../outputs/plots/pred_fall_fall.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_by_class(actual_class=\"fallen\", predicted_class=\"lying\", sample_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
